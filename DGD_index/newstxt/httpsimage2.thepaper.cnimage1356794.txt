MIT道德机器实验：当事故不可避免，自动驾驶汽车该怎么选
丁洪然
2018-12-12 	17:16
人工智能的快速发展，使得人们开始关注机器如何做出道德决策，以及如何量化社会对引导机器行为的伦理原则的预期。自动驾驶汽车已经在一些道路上开始巡航测试，这就需要人类社会就危及生命的交通事故无法避免地出现时应适用的原则达成一致。任何设计人工智能伦理的尝试都必须要了解公众的道德认知，因此，在这件事情上达成共识不仅需要工程师和伦理学家的探讨，更重要的是要听取未来消费者们的意见。
麻省理工学院于2016年部署了“道德机器”（Moral Machine）这一在线实验平台，旨在探索自动驾驶汽车面临的道德困境。“道德机器”（Moral Machine）被设计为一款多语言的在线“严肃游戏”，用于尽可能地在全世界范围内收集数据，通过了解公民希望自动驾驶汽车在不可避免的事故情况下如何解决道德难题来进行道德偏好的评估。实验结果于2018年10月发表在《自然》杂志网站上。
在道德机器的主界面上，用户可以看到不可避免的事故场景，根据自动驾驶汽车是突然转向还是继续行驶，从而导向两种不同的结果。事故场景是由道德机器产生的，遵循的探索策略集中于9个因素：保留人类（或宠物），保持直行（或转弯），保护行人（或乘客），保护更多的生命（或更少的生命），保护男人（或女人），保护年轻人（或年长者），保护合法过马路的行人（或乱穿马路者），保护健康者（或不健康者），保较高社会地位者（或较低社会地位者）。
道德机器困境。自动驾驶汽车突然刹车失灵。如果继续行驶，将会导致1名女运动员和1名男运动员死亡(左)。转弯会导致1名女运动员和1名过重的男人死亡 (右)。
基于最终收集到的来自233个国家和地区的数百万人用10种语言做出的4000万项决定 ，研究者从四个方面描述了实验结果：首先，总结全球的道德偏好；其次，根据受访者的人口统计数据记录个人偏好的变化；第三，报告了跨文化伦理差异，发现了三个主要的国家集群；第四，分析发现伦理差异与现代制度和深厚文化特质的相关性。
世界地图突出显示道德机器访客的位置。每个点代表至少一个访问者做出至少一个决定的位置(n = 3960万)。每个地点的访客数量或决策都没有表示出来。
全球首选项
调查结果显示，在道德机器实验中，呈现出三种十分强烈的偏好，分别为：保护人类而不是保护动物，保护更多的生命，保护年轻的生命。在研究者看来，这三个偏好应该为政策制定者着重考虑。
2017年，德国自动化和联网驾驶道德委员会(German Ethics Commission on Automated and Connected Driving)提出了一套伦理规则，是迄今为止唯一一次尝试为自主车辆的伦理选择提供官方指导的方针。在这一规则的第7条中明确指出，在进退两难的情况下，保护人类生命应该优先于保护其他动物生命，这一规则与调查结果中所显示的社会期望是一致的。这一规则的第9条规定，任何基于个人特征（如年龄）的区别都应被禁止，显然与调查中保护年轻生命的倾向相冲突的，显示出公众意见与专业观点之间的张力。
全球首选项图示。
个体差异
研究者通过进一步分析完成了关于年龄、教育、性别、收入、政治和宗教观点的回答来评估个体差异，以评估偏好是否受这六个特征的影响。
分析发现，个体变量对于9个因素中的任何一个都没有显著的影响。其中最显著的影响是由受访者的性别和宗教信仰所决定的。例如，男性受访者对女性的宽恕倾向较低、宗教信仰与人的宽容倾向有微弱的相关性。综合来说，这6个变量中没有一个将其亚群分裂为相反的效应方向，虽然存在一些个体差异（如男性和女性受访者都表示更倾向于保留女性，但后者显示出更强的偏好），但这对于政策制定者来说并不是关键信息。
文化集群
通过地理定位，研究者们可以识别道德机器应答者的居住国家，并寻找具有同质道德倾向的国家集群。通过分析，他们将这些国家分为三类：
第一个集群（研究者称之为西方集群）包含北方美国以及许多欧洲国家的新教、天主教和东正教基督教文化团体。该集群的内部结构也显示出显著的表面效度，一个子集群包含斯堪的纳维亚国家，一个子集群包含英联邦国家。
第二个集群（研究者称之为东方集群）包含了许多远东国家和地区，如日本和中国台湾等儒家文化团体，印尼等伊斯兰国家，以及巴基斯坦和沙特阿拉伯等等。
第三个集群（研究者称之为一个广泛的南方集群）包括中美洲和南美洲的拉丁美洲国家，除了一些部分受法国影响的国家。
这种集群模式表明，地理和文化上的接近可能使一些不同国家地区的人集中在对机器伦理的共同偏好上。然而，集群之间的差异可能会带来更大的问题：例如对于东部集群来说，保留较年轻角色而不是较老角色的偏好要低得多，而对于南部集群来说则要高得多，偏爱保留较高地位角色的情况也是如此；与其他两个集群相比，南部集群的国家对人类的保护相比对宠物的保护要弱得多。只有（微弱）倾向于不让行人超过乘客，（适度）倾向于不让合法的人超过非法的人，似乎在所有群体中都有相同程度的倾向性。
研究者认为，制造商和政策制定者们需要注意到，在他们设计人工智能系统和政策的国家和地区，人群的道德偏好是什么。虽然公众的道德偏好不一定是道德政策制定的决定性因素，但人们购买自动驾驶汽车并在容忍它们上路的意愿，将取决于所采用的道德规则的适应性。
国家层面的预测
通过进一步分析，研究者们还发现道德机器揭示的偏好与国家与地区间的文化和经济差异高度相关。两个国家或地区间在文化上越相似，这两国人在道德机器中的选择就越相似。
通过观察个人主义文化和集体主义文化之间的系统性差异，可以发现：来自个人主义文化的被调查者，强调每个人的独特价值，显示出偏好保护更多生命的强烈倾向；来自集体主义文化的被调查者，强调对较年长成员的尊重，显示出较不喜欢保护年轻的生命。由于政策制定者最需要考虑的是对保护多数人的偏好和对保护年轻人的偏爱，这种个人主义和集体主义文化的分裂可能会成为通用机器伦理的一个重要障碍。
政策制定者需要考虑那些非法过马路的人应该得到和合法过马路的行人同样的保护吗？与其他伦理优先事项相比，它们的保护优先性是否应该降低？通过观察发现，来自较贫穷和制度较弱的国家和地区的参与者对非法过马路的行人更宽容，大概是因为他们的遵守规则程度较低，对违规行为的惩罚较轻。
此外，来自经济上不平等的国家和地区的人们在道德机器上也不平等地对待富人和穷人，这种关系可以用不平等渗透到人们的道德偏好中来解释；在几乎所有国家和地区，参与者都表现出对女性的偏好，然而在妇女健康和生存前景较好的国家，这种偏好更强。
讨论
虽然到目前为止，人类从未允许一台机器在没有实时监控的情况，在一瞬间自主决定谁该生、谁该死，但不久的将来，它将发生在我们生活中最平凡的方面。在我们允许汽车做出道德决定之前，我们需要进行一次全球对话，向那些设计道德算法的公司以及将对它们进行监管的政策制定者表达我们的偏好。
道德机器的实验向我们展现了面对极端情况时，人类的三种强烈偏好，它们可以做为讨论通用机器伦理的基石。这一实验的雄心和目标是非典型的，研究者们通过部署一个病毒式的在线平台接触到大量参与者，此前没有任何研究尝试过在200多个国家使用九维实验设计来衡量道德偏好。虽然这一方法绕开了常规调研方法的困难，但是也导致无法保证样本完全匹配每个国家和地区的社会人口等缺陷。不过，换个角度来看，样本采集的范围是那群接近网络并对科技有兴趣的人，他们也更有可能参与到早期无人驾驶汽车使用当中，数据上对他们的偏重也并非毫无意义。
研究者指出，虽然我们可以让机器准确地遵循道德偏好，但我们无法达到普遍的共识，因为即便是通过道德机器表达的最强烈的偏好也显示出巨大的文化差异。不过，这并不意味着人类通往共识机器伦理的旅程从一开始就注定要失败，因为我们虽无法否认人类在道德领域中会经历内心冲突、人际分歧和文化差异这些实质性困难，但这些困难并非致命的，从数据中我们发现世界广泛地区还是表现出了一些相对一致的倾向性。
